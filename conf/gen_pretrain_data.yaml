input_file: ./data/i1.json #指定输入文档路径
output_file: ./data/o1.tfrecords #指定输出路径
vocab_file: vocab.txt #指定词典路径
max_seq_length: 128 #每一条训练数据（两句话）相加后的最大长度限制
short_seq_prob: 0.1 #为了缩小预训练和微调过程的差距，以此概率产生小于max_seq_length的训练数据
masked_lm_prob: 0.15 #一条训练数据产生mask的概率
max_predictions_per_seq: 20 #每一条训练数据mask的最大数量
random_seed: 12345
dupe_factor: 10 #对文档多次重复随机产生训练集，这意味着我们对同一份维基语料可以生成多次的数据，因为mask具有一定的随机性，所以当文本比较少时可以设置较大的重复次数来取得较多的训练样本。

#return
#tokens #分词结果
#input_ids #分词的id表示，并且用0补全序列长度到max_seq_length
#input_mask #用于记录实句长度。最后将不到最大长度的部分用0补齐
#segment_ids #句子编码 第一句为0 第二句为1
#masked_lm_positions #tokens中被mask的位置
#masked_lm_ids #tokens中被mask的原来的词对应的id
#masked_lm_labels #tokens中被mask的原来的词
#masked_lm_weights #mask位置的权重都为1，用于排除后续的“0”以便loss计算
#next_sentence_labels #第二句是否为随机生成的
#这里sentence的概念并不是自然意义上的一句话，而是连续的token，一般可能是很多句话而非一句话。
#如果采样出来的句子长度超过了限制，随机从头或者尾去掉比较长sentence的一些token。